{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFGbrPTHWvLjfidVG6fGaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nare-shh/Food-Image-Classification-using-Vision-Transformer-ViT-in-PyTorch/blob/main/ViT_Food_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoyabPMcgywd"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "7dzWcsFvh3DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "Lx2RBdYkh_Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "VLCeeH4wiWQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ],
      "metadata": {
        "id": "ETuziV0kibI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "id": "JrMoeBFrigOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "DnbHPSY9ijne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example values\n",
        "height = 224 # H (\"The training resolution is 224.\")\n",
        "width = 224 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 16 # P\n",
        "\n",
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
      ],
      "metadata": {
        "id": "Gx80RrFViqW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape (this is the size of a single image)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
      ],
      "metadata": {
        "id": "tBx906qMiyq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ],
      "metadata": {
        "id": "gy7-myHSi--v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=1,\n",
        "                        ncols=img_size // patch_size, # one column for each patch\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
        "    axs[i].set_xlabel(i+1) # set the label\n",
        "    axs[i].set_xticks([])\n",
        "    axs[i].set_yticks([])"
      ],
      "metadata": {
        "id": "nzucPKyKjCva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
        "                        ncols=img_size // patch_size,\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "\n",
        "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
        "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
        "                                        patch_width:patch_width+patch_size, # iterate through width\n",
        "                                        :]) # get all color channels\n",
        "\n",
        "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
        "        axs[i, j].set_ylabel(i+1,\n",
        "                             rotation=\"horizontal\",\n",
        "                             horizontalalignment=\"right\",\n",
        "                             verticalalignment=\"center\")\n",
        "        axs[i, j].set_xlabel(j+1)\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "        axs[i, j].label_outer()\n",
        "\n",
        "# Set a super title\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4fjUoX60jICb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=16\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
        "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
        "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ],
      "metadata": {
        "id": "yjzG0_lKjNSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ],
      "metadata": {
        "id": "wCe-qXDnja7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random 5 convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
        "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
        "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
      ],
      "metadata": {
        "id": "6Sr9-98NjeJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ],
      "metadata": {
        "id": "k2Hk4aMZjhi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Current tensor shape\n",
        "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "nmxFlTP-joW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create flatten layer\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ],
      "metadata": {
        "id": "hgzRLr2ajui7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # 1. View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ],
      "metadata": {
        "id": "9XeB2vQ7jxXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get flattened image patch embeddings in right shape\n",
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ],
      "metadata": {
        "id": "HtGWzqknj07F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "t5O9Q0N3j668"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the flattened feature map as a tensor\n",
        "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
      ],
      "metadata": {
        "id": "71X5ngEgkEaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "pXc87KMAkMFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size=16,\n",
        "                          embedding_dim=768)\n",
        "\n",
        "# Pass a single image through\n",
        "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
        "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
      ],
      "metadata": {
        "id": "UIzzM0clkQ38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random input sizes\n",
        "random_input_image = (1, 3, 224, 224)\n",
        "random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size"
      ],
      "metadata": {
        "id": "BduKCMsIkaB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the patch embedding and patch embedding shape\n",
        "print(patch_embedded_image)\n",
        "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "FU9_zjri6tO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dimension = patch_embedded_image.shape[-1]\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "                           requires_grad=True) # make sure the embedding is learnable\n",
        "\n",
        "# Show the first 10 examples of the class_token\n",
        "print(class_token[:, :, :10])\n",
        "\n",
        "# Print the class_token shape\n",
        "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "7p4CG9pu-DpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
        "                                                      dim=1) # concat on first dimension\n",
        "\n",
        "# Print the sequence of patch embeddings with the prepended class token embedding\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "_k80fwg2-HYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ],
      "metadata": {
        "id": "nhsSbiJp-UWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
        "print(position_embedding[:, :10, :10])\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "wm8FXiOX-aD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "print(patch_and_position_embedding)\n",
        "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "RcFXpffC-eoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Set patch size\n",
        "patch_size = 16\n",
        "\n",
        "# 2. Print shape of original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Get image tensor and add batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=768)\n",
        "\n",
        "# 5. Pass image through patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Create class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
        "                           requires_grad=True) # make sure it's learnable\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend class token embedding to patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# 9. Add position embedding to patch embedding with class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "id": "Upc0O42nB7vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "j4klVZ35CA7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
        "                                                             num_heads=12) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ],
      "metadata": {
        "id": "o22kJWMoCPZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AAdE0A3vCSaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768, # from Table 1\n",
        "                     mlp_size=3072, # from Table 1\n",
        "                     dropout=0.1) # from Table 3\n",
        "\n",
        "# Pass output of MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ],
      "metadata": {
        "id": "ejdoUP1JCVT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x\n",
        "\n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qWGEScxCCXoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TransformerEncoderBlock\n",
        "transformer_encoder_block = TransformerEncoderBlock()"
      ],
      "metadata": {
        "id": "18IYGQr3CbiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                                                             nhead=12, # Heads from Table 1 for ViT-Base\n",
        "                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n",
        "                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                                                             activation=\"gelu\", # GELU non-linear activation\n",
        "                                                             batch_first=True, # Do our batches come first?\n",
        "                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n",
        "\n",
        "torch_transformer_encoder_layer"
      ],
      "metadata": {
        "id": "UkQRxEl4CerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=16, # Patch size\n",
        "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "\n",
        "        # 3. Make the image size is divisible by the patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "raUPnpAYChkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of creating the class embedding and expanding over a batch dimension\n",
        "batch_size = 32\n",
        "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
        "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
        "\n",
        "# Print out the change in shapes\n",
        "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
        "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
      ],
      "metadata": {
        "id": "lIJkbfT3Clsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "id": "BxAs9mC2CoM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n"
      ],
      "metadata": {
        "id": "U0-YfqqzCqda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
        "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
        "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
        "\n",
        "# Setup the loss function for multi-class classification\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the seeds\n",
        "set_seeds()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "# Train the model and save the training results to a dictionary\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "y1ATa_ZZCuCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot our ViT model's loss curves\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "KRKPOSxxCwlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following requires torch v0.12+ and torchvision v0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "59Dc_2AoD48F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get pretrained weights for ViT-Base\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
        "\n",
        "# 2. Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# 3. Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
        "set_seeds()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
        "# pretrained_vit # uncomment for model output"
      ],
      "metadata": {
        "id": "o2UsJOaoEsHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import download_data\n",
        "\n",
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "MsjMIYqqEv4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and test directory paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "BqAbAH6cEyYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ],
      "metadata": {
        "id": "XI91HfTUE0Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
      ],
      "metadata": {
        "id": "EvK2w99AE2wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ],
      "metadata": {
        "id": "UAQ4Dka-E4tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ],
      "metadata": {
        "id": "Utbdzc8nE_UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=pretrained_vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")"
      ],
      "metadata": {
        "id": "yY5uo0TpFOyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "R_jq-5cJFRos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = image_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ],
      "metadata": {
        "id": "4UF5Jt5IFUrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Xz_IUxUFX2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}